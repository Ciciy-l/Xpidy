"""
Xpidy é…ç½®é©±åŠ¨çš„å‘½ä»¤è¡Œå·¥å…·
"""

import asyncio
import json
import sys
from pathlib import Path
from typing import Any, Dict, Optional

import click
from loguru import logger

from . import (ExtractionConfig, LLMConfig, Spider, SpiderConfig, quick_crawl,
               quick_crawl_batch)
from .utils import ContentUtils, URLUtils


@click.group()
@click.version_option(version="0.2.0")
def cli():
    """Xpidy - é…ç½®é©±åŠ¨çš„æ™ºèƒ½çˆ¬è™«å·¥å…·

    åŸºäº"é…ç½®å³æ–‡æ¡£"çš„è®¾è®¡ç†å¿µï¼Œé€šè¿‡é…ç½®æ–‡ä»¶å®šä¹‰çˆ¬å–ä»»åŠ¡ã€‚
    """
    pass


@cli.command()
@click.argument("config_file")
@click.option("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
@click.option("--dry-run", is_flag=True, help="é¢„è§ˆé…ç½®è€Œä¸æ‰§è¡Œ")
def run(config_file: str, output: Optional[str], dry_run: bool):
    """ä½¿ç”¨é…ç½®æ–‡ä»¶æ‰§è¡Œçˆ¬å–ä»»åŠ¡

    ç¤ºä¾‹é…ç½®æ–‡ä»¶ï¼š

    {
      "spider_config": {
        "headless": true,
        "timeout": 30000
      },
      "extraction_config": {
        "extract_text": true,
        "extract_links": true,
        "extract_images": true,
        "max_links": 20
      },
      "tasks": [
        {
          "url": "https://example.com",
          "name": "example_site"
        }
      ]
    }
    """

    async def run_task():
        try:
            # è¯»å–é…ç½®æ–‡ä»¶
            config_path = Path(config_file)
            if not config_path.exists():
                click.echo(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}", err=True)
                sys.exit(1)

            with open(config_path, "r", encoding="utf-8") as f:
                config = json.load(f)

            # è§£æé…ç½®
            spider_config = SpiderConfig(**config.get("spider_config", {}))
            extraction_config = ExtractionConfig(**config.get("extraction_config", {}))
            llm_config = None
            if "llm_config" in config:
                llm_config = LLMConfig(**config["llm_config"])

            tasks = config.get("tasks", [])

            if dry_run:
                click.echo("ğŸ” é…ç½®é¢„è§ˆ:")
                click.echo(f"  çˆ¬è™«é…ç½®: {spider_config}")
                click.echo(f"  æå–é…ç½®: {extraction_config}")
                if llm_config:
                    click.echo(f"  LLMé…ç½®: {llm_config}")
                click.echo(f"  ä»»åŠ¡æ•°é‡: {len(tasks)}")
                return

            if not tasks:
                click.echo("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰ä»»åŠ¡", err=True)
                sys.exit(1)

            # æ‰§è¡Œä»»åŠ¡
            click.echo(f"ğŸš€ å¼€å§‹æ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡")

            async with Spider(
                spider_config=spider_config,
                extraction_config=extraction_config,
                llm_config=llm_config,
            ) as spider:
                results = {}

                for i, task in enumerate(tasks, 1):
                    url = task["url"]
                    name = task.get("name", f"task_{i}")

                    try:
                        click.echo(f"ğŸ“¥ ({i}/{len(tasks)}) å¤„ç†: {name} - {url}")

                        # ä»»åŠ¡çº§åˆ«çš„é…ç½®è¦†ç›–
                        task_kwargs = task.get("options", {})

                        result = await spider.crawl(url, **task_kwargs)
                        results[name] = {
                            "url": url,
                            "success": result.success,
                            "data": result.to_dict(),
                        }

                        click.echo(f"âœ… å®Œæˆ: {name}")

                    except Exception as e:
                        click.echo(f"âŒ å¤±è´¥: {name} - {e}")
                        results[name] = {"url": url, "success": False, "error": str(e)}

                # ä¿å­˜ç»“æœ
                if output:
                    with open(output, "w", encoding="utf-8") as f:
                        json.dump(results, f, ensure_ascii=False, indent=2)
                    click.echo(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output}")
                else:
                    click.echo(json.dumps(results, ensure_ascii=False, indent=2))

                # æ˜¾ç¤ºæ€»ç»“
                successful = sum(1 for r in results.values() if r.get("success", False))
                click.echo(f"\nğŸ“Š æ‰§è¡Œæ€»ç»“: æˆåŠŸ {successful}/{len(tasks)} ä¸ªä»»åŠ¡")

        except Exception as e:
            click.echo(f"âŒ æ‰§è¡Œå¤±è´¥: {e}", err=True)
            sys.exit(1)

    asyncio.run(run_task())


@cli.command()
@click.argument(
    "template_name",
    type=click.Choice(["basic", "links", "images", "comprehensive", "llm"]),
)
@click.option("--output", "-o", default="xpidy_config.json", help="é…ç½®æ–‡ä»¶è¾“å‡ºè·¯å¾„")
def init(template_name: str, output: str):
    """ç”Ÿæˆé…ç½®æ–‡ä»¶æ¨¡æ¿

    å¯ç”¨æ¨¡æ¿:
    - basic: åŸºç¡€æ–‡æœ¬æå–
    - links: é“¾æ¥æå–
    - images: å›¾ç‰‡æå–
    - comprehensive: å…¨é¢æå–
    - llm: LLMå¢å¼ºæå–
    """
    templates = {
        "basic": {
            "spider_config": {"headless": True, "timeout": 30000, "stealth_mode": True},
            "extraction_config": {"extract_text": True, "extract_metadata": True},
            "tasks": [{"url": "https://example.com", "name": "example_basic"}],
        },
        "links": {
            "spider_config": {"headless": True, "timeout": 30000},
            "extraction_config": {
                "extract_text": True,
                "extract_links": True,
                "extract_internal_links": True,
                "extract_external_links": True,
                "max_links": 50,
            },
            "tasks": [
                {
                    "url": "https://example.com",
                    "name": "example_links",
                    "options": {"deduplicate": True, "include_media": False},
                }
            ],
        },
        "images": {
            "spider_config": {"headless": True, "timeout": 30000},
            "extraction_config": {
                "extract_text": True,
                "extract_images": True,
                "min_image_width": 100,
                "min_image_height": 100,
                "max_images": 20,
                "image_formats": ["jpg", "png", "gif"],
            },
            "tasks": [{"url": "https://example.com", "name": "example_images"}],
        },
        "comprehensive": {
            "spider_config": {"headless": True, "timeout": 30000, "stealth_mode": True},
            "extraction_config": {
                "extract_text": True,
                "extract_links": True,
                "extract_images": True,
                "extract_structured_data": True,
                "extract_tables": True,
                "extract_forms": True,
                "max_links": 100,
                "max_images": 30,
            },
            "tasks": [
                {"url": "https://example.com", "name": "comprehensive_extraction"}
            ],
        },
        "llm": {
            "spider_config": {"headless": True, "timeout": 30000},
            "extraction_config": {
                "extract_text": True,
                "enable_llm_processing": True,
                "structured_output": True,
            },
            "llm_config": {
                "provider": "openai",
                "model": "gpt-3.5-turbo",
                "api_key": "your-api-key-here",
                "temperature": 0.1,
            },
            "tasks": [
                {
                    "url": "https://example.com",
                    "name": "llm_extraction",
                    "options": {
                        "custom_prompt": "æå–è¿™ä¸ªé¡µé¢çš„ä¸»è¦ä¿¡æ¯",
                        "output_schema": {
                            "type": "object",
                            "properties": {
                                "title": {"type": "string"},
                                "summary": {"type": "string"},
                                "key_points": {
                                    "type": "array",
                                    "items": {"type": "string"},
                                },
                            },
                        },
                    },
                }
            ],
        },
    }

    template = templates.get(template_name)
    if not template:
        click.echo(f"âŒ æœªçŸ¥æ¨¡æ¿: {template_name}", err=True)
        sys.exit(1)

    # ä¿å­˜æ¨¡æ¿
    with open(output, "w", encoding="utf-8") as f:
        json.dump(template, f, ensure_ascii=False, indent=2)

    click.echo(f"âœ… å·²ç”Ÿæˆ {template_name} é…ç½®æ¨¡æ¿: {output}")
    click.echo(f"ğŸ”§ è¯·ç¼–è¾‘é…ç½®æ–‡ä»¶åä½¿ç”¨: xpidy run {output}")


@cli.command()
@click.argument("config_file")
def validate(config_file: str):
    """éªŒè¯é…ç½®æ–‡ä»¶"""
    try:
        config_path = Path(config_file)
        if not config_path.exists():
            click.echo(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}", err=True)
            sys.exit(1)

        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)

        # éªŒè¯é…ç½®ç»“æ„
        errors = []

        # éªŒè¯spider_config
        if "spider_config" in config:
            try:
                SpiderConfig(**config["spider_config"])
            except Exception as e:
                errors.append(f"spider_config é”™è¯¯: {e}")

        # éªŒè¯extraction_config
        if "extraction_config" in config:
            try:
                ExtractionConfig(**config["extraction_config"])
            except Exception as e:
                errors.append(f"extraction_config é”™è¯¯: {e}")

        # éªŒè¯llm_config
        if "llm_config" in config:
            try:
                LLMConfig(**config["llm_config"])
            except Exception as e:
                errors.append(f"llm_config é”™è¯¯: {e}")

        # éªŒè¯tasks
        tasks = config.get("tasks", [])
        if not tasks:
            errors.append("tasks ä¸èƒ½ä¸ºç©º")

        for i, task in enumerate(tasks):
            if "url" not in task:
                errors.append(f"ä»»åŠ¡ {i+1} ç¼ºå°‘ url å­—æ®µ")
            elif not URLUtils.is_valid_url(task["url"]):
                errors.append(f"ä»»åŠ¡ {i+1} çš„ URL æ— æ•ˆ: {task['url']}")

        if errors:
            click.echo("âŒ é…ç½®éªŒè¯å¤±è´¥:")
            for error in errors:
                click.echo(f"  - {error}")
            sys.exit(1)
        else:
            click.echo("âœ… é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡")
            click.echo(f"ğŸ“Š åŒ…å« {len(tasks)} ä¸ªä»»åŠ¡")

    except json.JSONDecodeError as e:
        click.echo(f"âŒ JSONæ ¼å¼é”™è¯¯: {e}", err=True)
        sys.exit(1)
    except Exception as e:
        click.echo(f"âŒ éªŒè¯å¤±è´¥: {e}", err=True)
        sys.exit(1)


@cli.command()
@click.argument("url")
@click.option("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
def quick(url: str, output: Optional[str]):
    """å¿«é€Ÿçˆ¬å–å•ä¸ªURLï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰"""

    async def run_quick():
        try:
            click.echo(f"ğŸš€ å¿«é€Ÿçˆ¬å–: {url}")

            result = await quick_crawl(url)

            output_data = json.dumps(result.to_dict(), ensure_ascii=False, indent=2)

            if output:
                with open(output, "w", encoding="utf-8") as f:
                    f.write(output_data)
                click.echo(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output}")
            else:
                click.echo(output_data)

            # æ˜¾ç¤ºæ‘˜è¦
            click.echo(f"\nğŸ“Š çˆ¬å–æ‘˜è¦:")
            click.echo(f"  æˆåŠŸ: {result.success}")
            click.echo(f"  å†…å®¹é•¿åº¦: {result.content_length}")
            if result.has_links:
                click.echo(f"  é“¾æ¥æ•°: {result.total_links}")
            if result.has_images:
                click.echo(f"  å›¾ç‰‡æ•°: {result.total_images}")

        except Exception as e:
            click.echo(f"âŒ å¿«é€Ÿçˆ¬å–å¤±è´¥: {e}", err=True)
            sys.exit(1)

    asyncio.run(run_quick())


# ä¿ç•™ä¸€äº›å®ç”¨çš„å·¥å…·å‘½ä»¤
@cli.command()
@click.argument("urls", nargs=-1)
def validate_urls(urls):
    """éªŒè¯URLçš„æœ‰æ•ˆæ€§"""
    if not urls:
        click.echo("è¯·æä¾›è‡³å°‘ä¸€ä¸ªURL")
        return

    click.echo("ğŸ” URLéªŒè¯ç»“æœ:")
    for url in urls:
        is_valid = URLUtils.is_valid_url(url)
        normalized = URLUtils.normalize_url(url) if is_valid else "æ— æ•ˆURL"
        domain = URLUtils.extract_domain(url) if is_valid else "N/A"

        status = "âœ…" if is_valid else "âŒ"
        click.echo(f"{status} {url}")
        click.echo(f"   æ ‡å‡†åŒ–: {normalized}")
        click.echo(f"   åŸŸå: {domain}")
        click.echo()


if __name__ == "__main__":
    cli()
