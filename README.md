# Xpidy - æ™ºèƒ½ç½‘é¡µæ•°æ®æå–æ¡†æ¶

ä¸€ä¸ªåŸºäºPlaywrightçš„ç°ä»£åŒ–æ™ºèƒ½çˆ¬è™«åº“ï¼Œé‡‡ç”¨"é…ç½®é©±åŠ¨"è®¾è®¡ç†å¿µã€‚

## âœ¨ ç‰¹æ€§

- ğŸ¯ **é…ç½®é©±åŠ¨** - åŸºäºé…ç½®æ–‡ä»¶çš„è‡ªåŠ¨åŒ–æ•°æ®æå–
- ğŸš€ **ç°ä»£åŒ–å·¥å…·é“¾** - åŸºäºuvåŒ…ç®¡ç†å™¨å’Œå¼‚æ­¥ç¼–ç¨‹
- ğŸ­ **Playwrighté©±åŠ¨** - æ”¯æŒJavaScriptæ¸²æŸ“å’ŒSPA
- ğŸ§  **æ™ºèƒ½æå–** - æ”¯æŒLLMå¢å¼ºçš„å†…å®¹ç†è§£
- ğŸ“‹ **å¤šç§æ•°æ®ç±»å‹** - æ–‡æœ¬ã€é“¾æ¥ã€å›¾ç‰‡ã€è¡¨æ ¼ã€è¡¨å•ç­‰
- ğŸ” **çµæ´»é€‰æ‹©å™¨** - æ”¯æŒCSSé€‰æ‹©å™¨å’ŒXPath
- âš¡ **é«˜æ€§èƒ½** - å¼‚æ­¥å¹¶å‘å¤„ç†å’Œæ™ºèƒ½ç¼“å­˜
- ğŸ›¡ï¸ **åçˆ¬è™«** - å†…ç½®éšèº«æ¨¡å¼å’Œéšæœºå»¶è¿Ÿ
- ğŸ“Š **ç›‘æ§ç»Ÿè®¡** - å®æ—¶æ€§èƒ½ç›‘æ§å’Œé”™è¯¯è¿½è¸ª
- ğŸ”§ **CLIå·¥å…·** - å‘½ä»¤è¡Œå·¥å…·æ”¯æŒé…ç½®æ–‡ä»¶æ¨¡å¼

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å®‰è£…uvï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
curl -LsSf https://astral.sh/uv/install.sh | sh

# å…‹éš†é¡¹ç›®
git clone <repo-url>
cd Xpidy

# å®‰è£…ä¾èµ–
uv sync

# å®‰è£…Playwrightæµè§ˆå™¨
uv run playwright install
```

### CLIä½¿ç”¨ï¼ˆæ¨èï¼‰

ä½¿ç”¨é…ç½®æ–‡ä»¶æ–¹å¼ï¼Œç¬¦åˆ"é…ç½®å³æ–‡æ¡£"è®¾è®¡ç†å¿µï¼š

```bash
# 1. ç”Ÿæˆé…ç½®æ¨¡æ¿
xpidy init basic --output my_config.json

# 2. éªŒè¯é…ç½®æ–‡ä»¶
xpidy validate my_config.json

# 3. æ‰§è¡Œçˆ¬å–ä»»åŠ¡
xpidy run my_config.json --output results.json

# 4. å¿«é€Ÿçˆ¬å–å•ä¸ªURL
xpidy quick https://example.com
```

**é…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼š**

```json
{
  "spider_config": {
    "headless": true,
    "timeout": 30000,
    "stealth_mode": true
  },
  "extraction_config": {
    "extract_text": true,
    "extract_links": true,
    "extract_images": true,
    "max_links": 50,
    "max_images": 20
  },
  "tasks": [
    {
      "url": "https://example.com",
      "name": "example_site"
    }
  ]
}
```

### ç¼–ç¨‹æ¥å£

```python
import asyncio
from xpidy import Spider, ExtractionConfig

async def main():
    # 1. å®šä¹‰æå–é…ç½®ï¼ˆé…ç½®å³æ–‡æ¡£ï¼‰
    config = ExtractionConfig(
        extract_text=True,      # æå–æ–‡æœ¬å†…å®¹
        extract_links=True,     # æå–é“¾æ¥
        extract_images=True,    # æå–å›¾ç‰‡
        max_links=20,          # æœ€å¤š20ä¸ªé“¾æ¥
        max_images=10          # æœ€å¤š10å¼ å›¾ç‰‡
    )
    
    # 2. åˆ›å»ºçˆ¬è™«å®ä¾‹
    async with Spider(extraction_config=config) as spider:
        # 3. ä¸€é”®æå–æ‰€æœ‰é…ç½®çš„æ•°æ®
        result = await spider.crawl("https://example.com")
        
        # 4. ä½¿ç”¨ä¾¿æ·å±æ€§è®¿é—®ç»“æœï¼ˆæœ‰IDEæ™ºèƒ½æç¤ºï¼‰
        print(f"ğŸ“° æ ‡é¢˜: {result.metadata.title}")
        print(f"ğŸ“ å†…å®¹: {result.content_length} å­—ç¬¦ï¼Œ{result.word_count} å­—")
        print(f"ğŸ”— é“¾æ¥: {result.total_links} ä¸ª")
        print(f"ğŸ–¼ï¸ å›¾ç‰‡: {result.total_images} å¼ ")
        
        # 5. ä½¿ç”¨ä¾¿æ·æ–¹æ³•è·å–ç‰¹å®šæ•°æ®
        if result.has_links:
            internal_links = result.get_internal_links()
            external_links = result.get_external_links()
            print(f"   å†…éƒ¨é“¾æ¥: {len(internal_links)} ä¸ª")
            print(f"   å¤–éƒ¨é“¾æ¥: {len(external_links)} ä¸ª")
        
        if result.has_images:
            large_images = result.get_large_images()
            jpg_images = result.get_images_by_format('jpg')
            print(f"   å¤§å›¾ç‰‡: {len(large_images)} å¼ ")
            print(f"   JPGå›¾ç‰‡: {len(jpg_images)} å¼ ")

asyncio.run(main())
```

## ğŸ“š CLIä½¿ç”¨æŒ‡å—

### é…ç½®æ¨¡æ¿

Xpidyæä¾›å¤šç§é¢„è®¾é…ç½®æ¨¡æ¿ï¼š

```bash
# åŸºç¡€æ–‡æœ¬æå–
xpidy init basic

# é“¾æ¥åˆ†æ
xpidy init links

# å›¾ç‰‡åˆ†æ
xpidy init images

# å…¨é¢æ•°æ®æå–
xpidy init comprehensive

# LLMå¢å¼ºæå–
xpidy init llm
```

### æ ¸å¿ƒCLIå‘½ä»¤

```bash
# ç”Ÿæˆé…ç½®æ¨¡æ¿
xpidy init <template> [--output config.json]

# æ‰§è¡Œé…ç½®ä»»åŠ¡
xpidy run <config_file> [--output results.json] [--dry-run]

# éªŒè¯é…ç½®æ–‡ä»¶
xpidy validate <config_file>

# å¿«é€Ÿçˆ¬å–
xpidy quick <url> [--output result.json]

# URLéªŒè¯å·¥å…·
xpidy validate-urls <url1> <url2> ...
```

### é…ç½®æ–‡ä»¶ç»“æ„

```json
{
  "spider_config": {
    "browser_type": "chromium",
    "headless": true,
    "timeout": 30000,
    "stealth_mode": true,
    "random_delay": true,
    "min_delay": 0.5,
    "max_delay": 2.0,
    "max_retries": 3,
    "user_agent": "custom-ua"
  },
  "extraction_config": {
    "extract_text": true,
    "extract_links": true,
    "extract_images": true,
    "extract_metadata": true,
    "extract_structured_data": true,
    "extract_tables": true,
    "extract_forms": true,
    "max_links": 100,
    "max_images": 50,
    "min_image_width": 50,
    "min_image_height": 50,
    "image_formats": ["jpg", "png", "gif", "webp"],
    "normalize_whitespace": true,
    "enable_deduplication": true
  },
  "llm_config": {
    "provider": "openai",
    "model": "gpt-3.5-turbo",
    "api_key": "your-api-key",
    "temperature": 0.1
  },
  "tasks": [
    {
      "url": "https://example.com",
      "name": "task_name",
      "options": {
        "custom_prompt": "æå–å…³é”®ä¿¡æ¯",
        "output_schema": {...}
      }
    }
  ]
}
```

## ğŸ“š ç¼–ç¨‹æ¥å£æŒ‡å—

### é…ç½®é©±åŠ¨çš„æ•°æ®æå–

```python
from xpidy import ExtractionConfig

# åŸºç¡€æ–‡æœ¬æå–
text_config = ExtractionConfig(
    extract_text=True,
    extract_metadata=True
)

# é“¾æ¥åˆ†æé…ç½®
link_config = ExtractionConfig(
    extract_text=True,
    extract_links=True,
    extract_internal_links=True,
    extract_external_links=True,
    max_links=50
)

# å›¾ç‰‡åˆ†æé…ç½®
image_config = ExtractionConfig(
    extract_text=True,
    extract_images=True,
    min_image_width=100,
    min_image_height=100,
    max_images=20,
    image_formats=["jpg", "png", "gif"]
)

# å…¨é¢åˆ†æé…ç½®
comprehensive_config = ExtractionConfig(
    extract_text=True,
    extract_links=True,
    extract_images=True,
    extract_structured_data=True,
    extract_tables=True,
    extract_forms=True,
    max_links=100,
    max_images=30
)
```

### æ ¸å¿ƒAPI

#### 1. åŸºç¡€çˆ¬å–

```python
async with Spider(extraction_config=config) as spider:
    # å•ä¸ªURL
    result = await spider.crawl(url)
    
    # æ‰¹é‡URL
    results = await spider.crawl_batch(urls, max_concurrent=5)
```

#### 2. é€‰æ‹©å™¨æå–

```python
# CSSé€‰æ‹©å™¨
css_selectors = {
    "title": "h1.title",
    "price": ".price",
    "description": ".description"
}
result = await spider.extract_with_selectors(url, css_selectors)

# XPathé€‰æ‹©å™¨
xpaths = {
    "title": "//h1[@class='title']/text()",
    "items": "//div[@class='item']",
    "links": "//a[@href]/@href"
}
result = await spider.extract_with_xpath(url, xpaths)
```

#### 3. LLMå¢å¼ºæå–

```python
from xpidy import LLMConfig

llm_config = LLMConfig(
    provider="openai",
    model="gpt-3.5-turbo",
    api_key="your-api-key"
)

# JSONæ¨¡å¼æå–
schema = {
    "type": "object",
    "properties": {
        "title": {"type": "string"},
        "price": {"type": "number"},
        "in_stock": {"type": "boolean"}
    }
}

async with Spider(extraction_config=config, llm_config=llm_config) as spider:
    result = await spider.extract_with_schema(url, schema)
```

### ä¾¿æ·å‡½æ•°

```python
from xpidy import quick_crawl, quick_crawl_batch

# å¿«é€Ÿå•æ¬¡çˆ¬å–
result = await quick_crawl(url, extraction_config=config)

# å¿«é€Ÿæ‰¹é‡çˆ¬å–
results = await quick_crawl_batch(urls, extraction_config=config)
```

### ç»“æœç»“æ„

Xpidy è¿”å›ç»“æ„åŒ–çš„ `CrawlResult` å¯¹è±¡ï¼Œæä¾›ç±»å‹å®‰å…¨å’ŒIDEæ™ºèƒ½æç¤ºï¼š

```python
# CrawlResult å¯¹è±¡çš„ä¾¿æ·å±æ€§
result = await spider.crawl(url)

# åŸºç¡€ä¿¡æ¯
print(f"URL: {result.url}")
print(f"æˆåŠŸçŠ¶æ€: {result.success}")
print(f"æ—¶é—´æˆ³: {result.timestamp}")

# ä¾¿æ·å±æ€§ï¼ˆæœ‰IDEæ™ºèƒ½æç¤ºï¼‰
print(f"æœ‰å†…å®¹: {result.has_content}")
print(f"æœ‰é“¾æ¥: {result.has_links}")
print(f"æœ‰å›¾ç‰‡: {result.has_images}")
print(f"å†…å®¹é•¿åº¦: {result.content_length}")
print(f"å­—æ•°: {result.word_count}")
print(f"æ€»é“¾æ¥æ•°: {result.total_links}")
print(f"æ€»å›¾ç‰‡æ•°: {result.total_images}")

# æå–å™¨çŠ¶æ€
print(f"å¯ç”¨çš„æå–å™¨: {result.enabled_extractors}")
print(f"æˆåŠŸçš„æå–å™¨: {result.successful_extractors}")
print(f"å¤±è´¥çš„æå–å™¨: {result.failed_extractors}")

# ä¾¿æ·æ•°æ®è®¿é—®æ–¹æ³•
internal_links = result.get_internal_links()
external_links = result.get_external_links()
large_images = result.get_large_images()
jpg_images = result.get_images_by_format('jpg')

# ç»Ÿè®¡ä¿¡æ¯
summary = result.get_summary()
detailed_stats = result.get_detailed_stats()

# å…¼å®¹æ€§ï¼šè½¬æ¢ä¸ºå­—å…¸ï¼ˆå¦‚æœéœ€è¦ï¼‰
result_dict = result.to_dict()
```

#### åŸå§‹æ•°æ®ç»“æ„

å¦‚æœéœ€è¦è®¿é—®åŸå§‹æ•°æ®ç»“æ„ï¼Œå¯ä»¥ç›´æ¥è®¿é—®å¯¹è±¡å±æ€§ï¼š

```python
# æ–‡æœ¬æ•°æ®
content = result.content
metadata = result.metadata
text_success = result.text_success

# é“¾æ¥æ•°æ®
links = result.links
link_stats = result.link_stats
links_success = result.links_success

# å›¾ç‰‡æ•°æ®
images = result.images
image_stats = result.image_stats
images_success = result.images_success

# ç»“æ„åŒ–æ•°æ®
structured_data = result.structured_data
structured_success = result.structured_success

# è¡¨æ ¼å’Œè¡¨å•æ•°æ®
tables = result.tables
forms = result.forms
```

## ğŸ”§ é«˜çº§é…ç½®

### çˆ¬è™«é…ç½®

```python
from xpidy import SpiderConfig

spider_config = SpiderConfig(
    browser_type="chromium",      # æµè§ˆå™¨ç±»å‹
    headless=True,                # æ— å¤´æ¨¡å¼
    timeout=30000,                # è¶…æ—¶æ—¶é—´(æ¯«ç§’)
    stealth_mode=True,            # éšèº«æ¨¡å¼
    random_delay=True,            # éšæœºå»¶è¿Ÿ
    min_delay=0.5,                # æœ€å°å»¶è¿Ÿ(ç§’)
    max_delay=2.0,                # æœ€å¤§å»¶è¿Ÿ(ç§’)
    max_retries=3,                # æœ€å¤§é‡è¯•æ¬¡æ•°
    user_agent="custom-ua"        # è‡ªå®šä¹‰UA
)
```

### æå–é…ç½®è¯¦è§£

```python
extraction_config = ExtractionConfig(
    # åŸºç¡€æå–
    extract_text=True,
    extract_links=True,
    extract_images=True,
    extract_metadata=True,
    extract_structured_data=True,
    extract_tables=True,
    extract_forms=True,
    
    # é“¾æ¥é…ç½®
    extract_internal_links=True,
    extract_external_links=True,
    max_links=100,
    link_filters=["*.pdf", "*.zip"],
    
    # å›¾ç‰‡é…ç½®
    min_image_width=50,
    min_image_height=50,
    max_images=50,
    image_formats=["jpg", "jpeg", "png", "gif", "webp"],
    
    # å†…å®¹å¤„ç†
    remove_scripts=True,
    remove_styles=True,
    normalize_whitespace=True,
    
    # LLMå¤„ç†
    enable_llm_processing=False,
    structured_output=False
)
```

## ğŸ› ï¸ å¼€å‘ç¯å¢ƒ

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate  # Linux/Mac
# æˆ–
.venv\Scripts\activate     # Windows

# è¿è¡Œæµ‹è¯•
uv run pytest

# è¿è¡Œç¤ºä¾‹
uv run python examples/01_quick_start.py
uv run python examples/02_practical_examples.py
uv run python examples/03_advanced_usage.py

# æµ‹è¯•CLIå·¥å…·
uv run xpidy init basic --output test_config.json
uv run xpidy validate test_config.json
uv run xpidy run test_config.json

# ä»£ç æ ¼å¼åŒ–
uvx run black .
uvx run isort .
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
Xpidy/
â”œâ”€â”€ xpidy/                      # ä¸»åŒ…
â”‚   â”œâ”€â”€ core/                   # æ ¸å¿ƒæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ spider.py           # ä¸»çˆ¬è™«ç±»
â”‚   â”‚   â”œâ”€â”€ config.py           # é…ç½®ç±»å®šä¹‰  
â”‚   â”‚   â”œâ”€â”€ results.py          # ç»“æœç±»å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ llm_processor.py    # LLMå¤„ç†å™¨
â”‚   â”‚   â””â”€â”€ __init__.py         # æ ¸å¿ƒæ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ extractors/             # æ•°æ®æå–å™¨æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ base_extractor.py   # æå–å™¨åŸºç±»
â”‚   â”‚   â”œâ”€â”€ text_extractor.py   # æ–‡æœ¬æå–å™¨
â”‚   â”‚   â”œâ”€â”€ link_extractor.py   # é“¾æ¥æå–å™¨
â”‚   â”‚   â”œâ”€â”€ image_extractor.py  # å›¾ç‰‡æå–å™¨
â”‚   â”‚   â”œâ”€â”€ data_extractor.py   # ç»“æ„åŒ–æ•°æ®æå–å™¨
â”‚   â”‚   â”œâ”€â”€ form_extractor.py   # è¡¨å•æå–å™¨
â”‚   â”‚   â””â”€â”€ __init__.py         # æå–å™¨æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ utils/                  # å·¥å…·æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ cache.py            # ç¼“å­˜ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ content_utils.py    # å†…å®¹å¤„ç†å·¥å…·
â”‚   â”‚   â”œâ”€â”€ url_utils.py        # URLå¤„ç†å·¥å…·
â”‚   â”‚   â”œâ”€â”€ stats.py            # ç»Ÿè®¡æ”¶é›†å™¨
â”‚   â”‚   â”œâ”€â”€ proxy.py            # ä»£ç†ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ retry.py            # é‡è¯•ç®¡ç†å™¨
â”‚   â”‚   â””â”€â”€ __init__.py         # å·¥å…·æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ cli.py                  # é…ç½®é©±åŠ¨çš„å‘½ä»¤è¡Œå·¥å…·
â”‚   â””â”€â”€ __init__.py             # åŒ…ä¸»å…¥å£
â”œâ”€â”€ examples/                   # ç¤ºä¾‹ä»£ç 
â”‚   â”œâ”€â”€ cli_config_examples/    # CLIé…ç½®æ–‡ä»¶ç¤ºä¾‹é›†åˆ
â”‚   â”œâ”€â”€ 01_quick_start.py       # å¿«é€Ÿå…¥é—¨ç¤ºä¾‹ï¼Œå±•ç¤ºåŸºç¡€æ•°æ®æå–åŠŸèƒ½
â”‚   â”œâ”€â”€ 02_practical_examples.py # å®ç”¨æ¡ˆä¾‹ç¤ºä¾‹ï¼Œå±•ç¤ºçœŸå®åœºæ™¯åº”ç”¨
â”‚   â””â”€â”€ 03_advanced_usage.py    # é«˜çº§ç”¨æ³•ç¤ºä¾‹ï¼Œå±•ç¤ºé€‰æ‹©å™¨ã€XPathç­‰é«˜çº§åŠŸèƒ½
â”œâ”€â”€ tests/                      # æµ‹è¯•æ–‡ä»¶
â”‚   â””â”€â”€ unit/                   # å•å…ƒæµ‹è¯•
â”‚       â””â”€â”€ test_utils.py       # å·¥å…·ç±»æµ‹è¯•
â”œâ”€â”€ .venv/                      # è™šæ‹Ÿç¯å¢ƒ (uvç®¡ç†)
â”œâ”€â”€ pyproject.toml              # é¡¹ç›®é…ç½®æ–‡ä»¶
â”œâ”€â”€ uv.lock                     # ä¾èµ–é”å®šæ–‡ä»¶
â”œâ”€â”€ .gitignore                  # Gitå¿½ç•¥æ–‡ä»¶
â””â”€â”€ README.md                   # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

### ğŸ¯ æ ¸å¿ƒæ¶æ„

#### 1. æ ¸å¿ƒæ¨¡å— (`xpidy/core/`)
- **spider.py**: ä¸»çˆ¬è™«ç±»ï¼Œæä¾›ç»Ÿä¸€çš„çˆ¬å–æ¥å£
- **config.py**: é…ç½®ç±»å®šä¹‰ï¼ŒåŒ…æ‹¬çˆ¬è™«é…ç½®ã€æå–é…ç½®å’ŒLLMé…ç½®
- **results.py**: ç»“æœç±»å®šä¹‰ï¼Œæä¾›ç±»å‹å®‰å…¨çš„è¿”å›ç»“æœ
- **llm_processor.py**: LLMå¤„ç†å™¨ï¼Œæ”¯æŒOpenAIã€Claudeç­‰æ¨¡å‹

#### 2. æå–å™¨æ¨¡å— (`xpidy/extractors/`)
- **base_extractor.py**: æ‰€æœ‰æå–å™¨çš„åŸºç±»ï¼ŒåŒ…å«é€šç”¨å¤„ç†é€»è¾‘
- **text_extractor.py**: æ–‡æœ¬å†…å®¹æå–ï¼Œæ”¯æŒæ™ºèƒ½æ¸…ç†å’ŒLLMå¤„ç†
- **link_extractor.py**: é“¾æ¥æå–å’Œåˆ†æï¼Œæ”¯æŒå†…å¤–éƒ¨é“¾æ¥åˆ†ç±»
- **image_extractor.py**: å›¾ç‰‡æå–å’Œå…ƒæ•°æ®åˆ†æ
- **data_extractor.py**: ç»“æ„åŒ–æ•°æ®æå–ï¼Œæ”¯æŒJSON-LDå’Œå¾®æ•°æ®
- **form_extractor.py**: è¡¨å•å­—æ®µæå–å’Œåˆ†æ

#### 3. å·¥å…·æ¨¡å— (`xpidy/utils/`)
- **cache.py**: æ™ºèƒ½ç¼“å­˜ç®¡ç†ï¼Œæ”¯æŒå†…å­˜å’Œç£ç›˜ç¼“å­˜
- **content_utils.py**: å†…å®¹å¤„ç†å·¥å…·ï¼Œæ–‡æœ¬æ¸…ç†å’Œæ ¼å¼åŒ–
- **url_utils.py**: URLå¤„ç†å·¥å…·ï¼ŒåŸŸåæå–å’Œé“¾æ¥è§„èŒƒåŒ–
- **stats.py**: æ€§èƒ½ç»Ÿè®¡æ”¶é›†å™¨ï¼Œç›‘æ§çˆ¬å–æ•ˆç‡
- **proxy.py**: ä»£ç†ç®¡ç†å™¨ï¼Œæ”¯æŒè½®æ¢å’Œæ•…éšœè½¬ç§»
- **retry.py**: é‡è¯•ç®¡ç†å™¨ï¼Œæ™ºèƒ½é‡è¯•æœºåˆ¶

#### 4. å‘½ä»¤è¡Œå·¥å…· (`xpidy/cli.py`)
- åŸºäºé…ç½®æ–‡ä»¶çš„CLIå·¥å…·ï¼Œæ”¯æŒæ¨¡æ¿ç”Ÿæˆã€é…ç½®éªŒè¯ã€ä»»åŠ¡æ‰§è¡Œ
- ç¬¦åˆ"é…ç½®å³æ–‡æ¡£"è®¾è®¡ç†å¿µï¼Œæ— éœ€è®°å¿†å¤æ‚å‘½ä»¤å‚æ•°

#### 5. ç¤ºä¾‹ä»£ç  (`examples/`)
- **cli_config_examples/**: CLIé…ç½®æ–‡ä»¶ç¤ºä¾‹é›†åˆ
- **01_quick_start.py**: å¿«é€Ÿå…¥é—¨ç¤ºä¾‹ï¼Œå±•ç¤ºåŸºç¡€æ•°æ®æå–åŠŸèƒ½
- **02_practical_examples.py**: å®ç”¨æ¡ˆä¾‹ç¤ºä¾‹ï¼Œå±•ç¤ºçœŸå®åœºæ™¯åº”ç”¨
- **03_advanced_usage.py**: é«˜çº§ç”¨æ³•ç¤ºä¾‹ï¼Œå±•ç¤ºé€‰æ‹©å™¨ã€XPathç­‰é«˜çº§åŠŸèƒ½

## ğŸ¯ è®¾è®¡ç†å¿µ

### é…ç½®å³æ–‡æ¡£
é€šè¿‡é…ç½®ç±»å’Œé…ç½®æ–‡ä»¶æ˜ç¡®è¡¨è¾¾æå–æ„å›¾ï¼Œä»£ç å³æ–‡æ¡£ï¼Œæ˜“äºç†è§£å’Œç»´æŠ¤ã€‚CLIå·¥å…·å®Œå…¨åŸºäºé…ç½®æ–‡ä»¶ï¼Œé¿å…å¤æ‚çš„å‘½ä»¤è¡Œå‚æ•°ã€‚

### ç®€åŒ–API
åªéœ€å…³æ³¨é…ç½®å’Œæ ¸å¿ƒcrawlæ–¹æ³•ï¼Œé¿å…å¤æ‚çš„APIå­¦ä¹ æˆæœ¬ã€‚ç»Ÿä¸€çš„é€šç”¨å¤„ç†é€»è¾‘ï¼Œå‡å°‘ä»£ç é‡å¤ã€‚

### æ€§èƒ½ä¼˜åŒ–
å…±äº«é¡µé¢ä¼šè¯ï¼Œå‡å°‘é‡å¤åŠ è½½ï¼Œæ”¯æŒå¹¶å‘å¤„ç†ã€‚ç»Ÿä¸€çš„è¿‡æ»¤å’Œå»é‡æœºåˆ¶ï¼Œæé«˜å¤„ç†æ•ˆç‡ã€‚

### å®¹é”™æ€§å¼º
å•ä¸ªæå–å™¨å¤±è´¥ä¸å½±å“å…¶ä»–æå–å™¨ï¼Œæä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ã€‚

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

---

**Xpidy - è®©ç½‘é¡µæ•°æ®æå–å˜å¾—ç®€å•è€Œå¼ºå¤§ï¼** ğŸ•·ï¸âœ¨
