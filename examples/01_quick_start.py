#!/usr/bin/env python3
"""
Xpidy å¿«é€Ÿå…¥é—¨ç¤ºä¾‹
å±•ç¤ºåŸºæœ¬çš„ç½‘é¡µæ•°æ®æå–åŠŸèƒ½
"""

import asyncio

from xpidy import ExtractionConfig, Spider


async def basic_text_extraction():
    """æœ€ç®€å•çš„æ–‡æœ¬æå–"""
    print("ğŸš€ åŸºæœ¬æ–‡æœ¬æå–")
    print("-" * 40)

    config = ExtractionConfig(extract_text=True)

    async with Spider(extraction_config=config) as spider:
        result = await spider.crawl("https://example.com")

        print(f"æ ‡é¢˜: {result.metadata.title}")
        print(f"å†…å®¹é•¿åº¦: {result.content_length} å­—ç¬¦")
        print(f"æå–æˆåŠŸ: {result.text_success}")


async def extract_links_and_images():
    """æå–é“¾æ¥å’Œå›¾ç‰‡"""
    print("\nğŸ”— é“¾æ¥å’Œå›¾ç‰‡æå–")
    print("-" * 40)

    config = ExtractionConfig(
        extract_text=True,
        extract_links=True,
        extract_images=True,
        max_links=10,
        max_images=5,
    )

    async with Spider(extraction_config=config) as spider:
        result = await spider.crawl("https://httpbin.org/html")

        print(f"é¡µé¢æ ‡é¢˜: {result.metadata.title}")
        print(f"æ–‡æœ¬å†…å®¹: {result.content_length} å­—ç¬¦")
        print(f"é“¾æ¥æ€»æ•°: {result.total_links}")
        print(f"å›¾ç‰‡æ€»æ•°: {result.total_images}")

        if result.has_links:
            print(f"å†…éƒ¨é“¾æ¥: {len(result.get_internal_links())}")
            print(f"å¤–éƒ¨é“¾æ¥: {len(result.get_external_links())}")


async def batch_processing():
    """æ‰¹é‡å¤„ç†å¤šä¸ªURL"""
    print("\nğŸ“¦ æ‰¹é‡å¤„ç†")
    print("-" * 40)

    urls = ["https://example.com", "https://httpbin.org/html"]

    config = ExtractionConfig(extract_text=True, extract_links=True)

    async with Spider(extraction_config=config) as spider:
        results = await spider.crawl_batch(urls, max_concurrent=2)

        for i, result in enumerate(results, 1):
            print(f"ç»“æœ {i}: {result.url}")
            print(f"  æˆåŠŸ: {result.success}")
            print(f"  å†…å®¹: {result.content_length} å­—ç¬¦")
            print(f"  é“¾æ¥: {result.total_links} ä¸ª")


async def using_selectors():
    """ä½¿ç”¨é€‰æ‹©å™¨æå–ç‰¹å®šæ•°æ®"""
    print("\nğŸ¯ é€‰æ‹©å™¨æå–")
    print("-" * 40)

    async with Spider() as spider:
        # CSSé€‰æ‹©å™¨
        css_selectors = {"title": "title, h1", "paragraphs": "p", "links": "a[href]"}

        result = await spider.extract_with_selectors(
            "https://example.com", css_selectors
        )

        print(f"æå–æˆåŠŸ: {result.success}")
        print(f"æå–çš„æ•°æ®å­—æ®µ: {list(result.structured_data.keys())}")

        # æ˜¾ç¤ºæå–çš„å†…å®¹
        for key, value in result.structured_data.items():
            if value:
                if isinstance(value, list):
                    print(f"{key}: {len(value)} ä¸ªå…ƒç´ ")
                else:
                    print(f"{key}: {str(value)[:50]}...")


async def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸ•·ï¸ Xpidy å¿«é€Ÿå…¥é—¨ç¤ºä¾‹")
    print("=" * 50)

    try:
        await basic_text_extraction()
        await extract_links_and_images()
        await batch_processing()
        await using_selectors()

        print("\nâœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("\nğŸ’¡ æ¥ä¸‹æ¥å¯ä»¥æŸ¥çœ‹æ›´å¤šç¤ºä¾‹:")
        print("   - 02_practical_examples.py (å®ç”¨æ¡ˆä¾‹)")
        print("   - 03_advanced_usage.py (é«˜çº§ç”¨æ³•)")

    except Exception as e:
        print(f"âŒ ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")


if __name__ == "__main__":
    asyncio.run(main())
