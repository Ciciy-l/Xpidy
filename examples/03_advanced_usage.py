#!/usr/bin/env python3
"""
Xpidy é«˜çº§ç”¨æ³•ç¤ºä¾‹
å±•ç¤ºé€‰æ‹©å™¨ã€XPathã€é…ç½®ä¼˜åŒ–ç­‰é«˜çº§åŠŸèƒ½
"""

import asyncio

from xpidy import ExtractionConfig, Spider, SpiderConfig


async def advanced_selectors():
    """é«˜çº§é€‰æ‹©å™¨ä½¿ç”¨"""
    print("ğŸ¯ é«˜çº§é€‰æ‹©å™¨ç¤ºä¾‹")
    print("-" * 40)

    async with Spider() as spider:
        # å¤æ‚çš„CSSé€‰æ‹©å™¨
        css_selectors = {
            "title": "title, h1",
            "meta_description": "meta[name='description']",
            "all_headings": "h1, h2, h3, h4, h5, h6",
            "navigation_links": "nav a, header a, .nav a",
            "content_paragraphs": "main p, article p, .content p",
            "external_links": "a[href^='http']:not([href*='example.com'])",
            "images_with_alt": "img[alt]",
            "form_inputs": "input[type], textarea, select",
        }

        result = await spider.extract_with_selectors(
            "https://example.com", css_selectors
        )

        print("CSSé€‰æ‹©å™¨æå–ç»“æœ:")
        for field, data in result.structured_data.items():
            if data:
                if isinstance(data, list):
                    print(f"  {field}: æ‰¾åˆ° {len(data)} ä¸ªå…ƒç´ ")
                    # æ˜¾ç¤ºå‰å‡ ä¸ªå…ƒç´ çš„é¢„è§ˆ
                    if len(data) > 0:
                        preview = (
                            str(data[0])[:30] + "..."
                            if len(str(data[0])) > 30
                            else str(data[0])
                        )
                        print(f"    ç¤ºä¾‹: {preview}")
                else:
                    preview = (
                        str(data)[:50] + "..." if len(str(data)) > 50 else str(data)
                    )
                    print(f"  {field}: {preview}")
            else:
                print(f"  {field}: æœªæ‰¾åˆ°")


async def xpath_extraction():
    """XPath æå–ç¤ºä¾‹"""
    print("\nğŸ” XPath æå–ç¤ºä¾‹")
    print("-" * 40)

    async with Spider() as spider:
        # XPath é€‰æ‹©å™¨
        xpaths = {
            "page_title": "//title/text()",
            "all_text_nodes": "//text()[normalize-space()]",
            "link_urls": "//a/@href",
            "link_texts": "//a/text()",
            "image_sources": "//img/@src",
            "meta_keywords": "//meta[@name='keywords']/@content",
            "all_headings": "//h1 | //h2 | //h3",
            "paragraph_count": "count(//p)",
            "first_paragraph": "(//p)[1]/text()",
        }

        result = await spider.extract_with_xpath("https://example.com", xpaths)

        print("XPathæå–ç»“æœ:")
        for field, data in result.structured_data.items():
            if data:
                if isinstance(data, list):
                    print(f"  {field}: {len(data)} ä¸ªç»“æœ")
                    if field == "all_text_nodes" and len(data) > 5:
                        print(f"    å‰5ä¸ª: {[str(t)[:20] + '...' for t in data[:5]]}")
                    elif len(data) <= 3:
                        print(f"    å†…å®¹: {data}")
                else:
                    print(f"  {field}: {data}")
            else:
                print(f"  {field}: æœªæ‰¾åˆ°")


async def custom_configuration():
    """è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹"""
    print("\nâš™ï¸ è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹")
    print("-" * 40)

    # é«˜åº¦è‡ªå®šä¹‰çš„æå–é…ç½®
    config = ExtractionConfig(
        # åŸºç¡€æå–
        extract_text=True,
        extract_links=True,
        extract_images=True,
        extract_metadata=True,
        extract_structured_data=True,
        # é“¾æ¥é…ç½®
        max_links=30,
        extract_internal_links=True,
        extract_external_links=True,
        link_filters=["*.pdf", "*.doc", "*.zip"],
        # å›¾ç‰‡é…ç½®
        max_images=20,
        min_image_width=50,
        min_image_height=50,
        image_formats=["jpg", "jpeg", "png", "gif", "webp"],
        # å†…å®¹å¤„ç†
        remove_scripts=True,
        remove_styles=True,
        normalize_whitespace=True,
    )

    # è‡ªå®šä¹‰çˆ¬è™«é…ç½®
    spider_config = SpiderConfig(
        browser_type="chromium",
        headless=True,
        stealth_mode=True,
        timeout=20000,
        random_delay=True,
        min_delay=0.5,
        max_delay=1.5,
        max_retries=2,
    )

    async with Spider(config, spider_config) as spider:
        result = await spider.crawl("https://httpbin.org/html")

        print(f"é…ç½®åŒ–æå–ç»“æœ:")
        print(f"  é¡µé¢: {result.url}")
        print(f"  æˆåŠŸ: {result.success}")
        print(f"  å†…å®¹: {result.content_length} å­—ç¬¦")
        print(f"  é“¾æ¥: {result.total_links} ä¸ª")
        print(f"  å›¾ç‰‡: {result.total_images} å¼ ")

        # è¯¦ç»†ç»Ÿè®¡
        stats = result.get_detailed_stats()
        print(f"  è¯¦ç»†ç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"    {key}: {value}")


async def error_handling_and_retries():
    """é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
    print("\nğŸ”„ é”™è¯¯å¤„ç†å’Œé‡è¯•")
    print("-" * 40)

    # é…ç½®é‡è¯•æœºåˆ¶
    spider_config = SpiderConfig(
        timeout=5000, max_retries=3, random_delay=False  # çŸ­è¶…æ—¶æ—¶é—´ï¼Œå®¹æ˜“è§¦å‘é‡è¯•
    )

    # æµ‹è¯•URLåˆ—è¡¨ï¼ˆåŒ…å«ä¸€äº›å¯èƒ½å¤±è´¥çš„URLï¼‰
    test_urls = [
        "https://example.com",
        "https://httpbin.org/delay/10",  # å»¶è¿Ÿ10ç§’ï¼Œå¯èƒ½è¶…æ—¶
        "https://httpbin.org/status/404",  # 404é”™è¯¯
        "https://invalid-domain-12345.com",  # æ— æ•ˆåŸŸå
        "https://httpbin.org/html",
    ]

    config = ExtractionConfig(extract_text=True, extract_metadata=True)

    async with Spider(config, spider_config) as spider:
        results = await spider.crawl_batch(test_urls, max_concurrent=2)

        success_count = 0
        for i, result in enumerate(results):
            url = test_urls[i]
            if result.success:
                success_count += 1
                print(f"âœ… {url}")
                print(f"   å†…å®¹: {result.content_length} å­—ç¬¦")
            else:
                print(f"âŒ {url}")
                print(f"   é”™è¯¯: {result.error}")

        print(f"\nç»Ÿè®¡: {success_count}/{len(test_urls)} æˆåŠŸ")


async def performance_comparison():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("\nâš¡ æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("-" * 40)

    urls = ["https://example.com", "https://httpbin.org/html"]

    # æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½å·®å¼‚
    configs = [
        ("åŸºç¡€é…ç½®", ExtractionConfig(extract_text=True)),
        (
            "å®Œæ•´é…ç½®",
            ExtractionConfig(
                extract_text=True,
                extract_links=True,
                extract_images=True,
                extract_metadata=True,
                max_links=20,
                max_images=10,
            ),
        ),
        ("æœ€å°é…ç½®", ExtractionConfig(extract_metadata=True)),
    ]

    import time

    for config_name, config in configs:
        start_time = time.time()

        async with Spider(extraction_config=config) as spider:
            results = await spider.crawl_batch(urls, max_concurrent=2)

        duration = time.time() - start_time
        success_count = sum(1 for r in results if r.success)
        total_content = sum(r.content_length for r in results if r.success)

        print(f"{config_name}:")
        print(f"  æ—¶é—´: {duration:.2f}ç§’")
        print(f"  æˆåŠŸ: {success_count}/{len(urls)}")
        print(f"  å†…å®¹: {total_content} å­—ç¬¦")
        print(f"  é€Ÿåº¦: {total_content/duration:.0f} å­—ç¬¦/ç§’")


async def data_filtering_and_processing():
    """æ•°æ®è¿‡æ»¤å’Œå¤„ç†"""
    print("\nğŸ”§ æ•°æ®è¿‡æ»¤å’Œå¤„ç†")
    print("-" * 40)

    config = ExtractionConfig(
        extract_text=True,
        extract_links=True,
        extract_images=True,
        max_links=50,
        max_images=20,
    )

    async with Spider(extraction_config=config) as spider:
        result = await spider.crawl("https://httpbin.org/html")

        if result.success:
            print(f"åŸå§‹æ•°æ®:")
            print(f"  æ€»é“¾æ¥: {result.total_links}")
            print(f"  æ€»å›¾ç‰‡: {result.total_images}")

            # ä½¿ç”¨ä¾¿æ·æ–¹æ³•è¿‡æ»¤æ•°æ®
            if result.has_links:
                internal_links = result.get_internal_links()
                external_links = result.get_external_links()

                print(f"\né“¾æ¥åˆ†æ:")
                print(f"  å†…éƒ¨é“¾æ¥: {len(internal_links)}")
                print(f"  å¤–éƒ¨é“¾æ¥: {len(external_links)}")

                # æ˜¾ç¤ºé“¾æ¥è¯¦æƒ…
                if internal_links:
                    print(f"  å†…éƒ¨é“¾æ¥ç¤ºä¾‹: {internal_links[0]['url']}")
                if external_links:
                    print(f"  å¤–éƒ¨é“¾æ¥ç¤ºä¾‹: {external_links[0]['url']}")

            if result.has_images:
                large_images = result.get_large_images(min_width=100, min_height=100)
                small_images = [
                    img
                    for img in result.images
                    if img.get("width", 0) < 100 or img.get("height", 0) < 100
                ]

                print(f"\nå›¾ç‰‡åˆ†æ:")
                print(f"  å¤§å›¾ç‰‡: {len(large_images)}")
                print(f"  å°å›¾ç‰‡: {len(small_images)}")

                # æŒ‰æ ¼å¼åˆ†ç±»
                jpg_images = result.get_images_by_format("jpg")
                png_images = result.get_images_by_format("png")
                print(f"  JPGæ ¼å¼: {len(jpg_images)}")
                print(f"  PNGæ ¼å¼: {len(png_images)}")


async def result_analysis():
    """ç»“æœåˆ†æå’Œç»Ÿè®¡"""
    print("\nğŸ“Š ç»“æœåˆ†æå’Œç»Ÿè®¡")
    print("-" * 40)

    config = ExtractionConfig(
        extract_text=True,
        extract_links=True,
        extract_images=True,
        extract_metadata=True,
    )

    async with Spider(extraction_config=config) as spider:
        result = await spider.crawl("https://httpbin.org/html")

        if result.success:
            # åŸºæœ¬ç»Ÿè®¡
            print(f"åŸºæœ¬ä¿¡æ¯:")
            print(f"  URL: {result.url}")
            print(f"  æ—¶é—´æˆ³: {result.timestamp}")
            print(f"  å†…å®¹é•¿åº¦: {result.content_length}")
            print(f"  è¯æ•°: {result.word_count}")

            # æå–å™¨çŠ¶æ€
            print(f"\næå–å™¨çŠ¶æ€:")
            print(f"  å¯ç”¨: {result.enabled_extractors}")
            print(f"  æˆåŠŸ: {result.successful_extractors}")
            print(f"  å¤±è´¥: {result.failed_extractors}")

            # è·å–æ‘˜è¦
            summary = result.get_summary()
            print(f"\né¡µé¢æ‘˜è¦:")
            for key, value in summary.items():
                print(f"  {key}: {value}")

            # è¯¦ç»†ç»Ÿè®¡
            detailed_stats = result.get_detailed_stats()
            print(f"\nè¯¦ç»†ç»Ÿè®¡:")
            for category, stats in detailed_stats.items():
                print(f"  {category}:")
                if isinstance(stats, dict):
                    for key, value in stats.items():
                        print(f"    {key}: {value}")
                else:
                    print(f"    {stats}")


async def main():
    """è¿è¡Œæ‰€æœ‰é«˜çº§ç”¨æ³•ç¤ºä¾‹"""
    print("ğŸš€ Xpidy é«˜çº§ç”¨æ³•æ¼”ç¤º")
    print("=" * 50)

    try:
        await advanced_selectors()
        await xpath_extraction()
        await custom_configuration()
        await error_handling_and_retries()
        await performance_comparison()
        await data_filtering_and_processing()
        await result_analysis()

        print("\nğŸŠ æ‰€æœ‰é«˜çº§ç”¨æ³•æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ¯ æœ¬ç¤ºä¾‹å±•ç¤ºäº†ä»¥ä¸‹é«˜çº§ç‰¹æ€§:")
        print("   - å¤æ‚CSSé€‰æ‹©å™¨å’ŒXPath")
        print("   - è¯¦ç»†çš„é…ç½®è‡ªå®šä¹‰")
        print("   - é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")
        print("   - æ€§èƒ½ä¼˜åŒ–å’Œå¯¹æ¯”")
        print("   - æ•°æ®è¿‡æ»¤å’Œå¤„ç†")
        print("   - ç»“æœåˆ†æå’Œç»Ÿè®¡")

    except Exception as e:
        print(f"âŒ é«˜çº§ç”¨æ³•æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
