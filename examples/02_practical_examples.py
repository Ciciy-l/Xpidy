#!/usr/bin/env python3
"""
Xpidy å®ç”¨æ¡ˆä¾‹ç¤ºä¾‹
å±•ç¤ºçœŸå®åœºæ™¯ä¸­çš„æ•°æ®æå–åº”ç”¨
"""

import asyncio

from xpidy import ExtractionConfig, Spider, SpiderConfig


async def news_article_extraction():
    """æ–°é—»æ–‡ç« æå–æ¡ˆä¾‹"""
    print("ğŸ“° æ–°é—»æ–‡ç« æå–")
    print("-" * 40)

    config = ExtractionConfig(
        extract_text=True,
        extract_links=True,
        extract_images=True,
        max_links=20,
        max_images=10,
    )

    # ä½¿ç”¨è‡ªå®šä¹‰çˆ¬è™«é…ç½®
    spider_config = SpiderConfig(headless=True, stealth_mode=True, timeout=15000)

    async with Spider(config, spider_config) as spider:
        # ç¤ºä¾‹ä½¿ç”¨ httpbin.org çš„ HTML é¡µé¢
        result = await spider.crawl("https://httpbin.org/html")

        print(f"æ–‡ç« æ ‡é¢˜: {result.metadata.title}")
        print(f"æ–‡ç« é•¿åº¦: {result.content_length} å­—ç¬¦")
        print(f"ç›¸å…³é“¾æ¥: {result.total_links} ä¸ª")
        print(f"é…å›¾æ•°é‡: {result.total_images} å¼ ")

        # åˆ†æé“¾æ¥ç±»å‹
        if result.has_links:
            internal = result.get_internal_links()
            external = result.get_external_links()
            print(f"ç«™å†…é“¾æ¥: {len(internal)} | ç«™å¤–é“¾æ¥: {len(external)}")


async def e_commerce_product_analysis():
    """ç”µå•†äº§å“é¡µé¢åˆ†æ"""
    print("\nğŸ›’ ç”µå•†äº§å“åˆ†æ")
    print("-" * 40)

    config = ExtractionConfig(
        extract_text=True,
        extract_images=True,
        extract_tables=True,
        extract_forms=True,
        min_image_width=100,
        min_image_height=100,
        max_images=15,
    )

    async with Spider(extraction_config=config) as spider:
        result = await spider.crawl("https://example.com")

        print(f"äº§å“é¡µé¢: {result.url}")
        print(f"é¡µé¢å†…å®¹: {result.content_length} å­—ç¬¦")

        # äº§å“å›¾ç‰‡åˆ†æ
        if result.has_images:
            large_images = result.get_large_images(min_width=200, min_height=200)
            jpg_images = result.get_images_by_format("jpg")
            png_images = result.get_images_by_format("png")

            print(f"äº§å“å›¾ç‰‡: {result.total_images} å¼ ")
            print(f"  é«˜æ¸…å›¾ç‰‡: {len(large_images)} å¼ ")
            print(f"  JPGæ ¼å¼: {len(jpg_images)} å¼ ")
            print(f"  PNGæ ¼å¼: {len(png_images)} å¼ ")

        # è¡¨æ ¼å’Œè¡¨å•åˆ†æ
        if result.has_tables:
            print(f"è§„æ ¼è¡¨æ ¼: {len(result.tables)} ä¸ª")

        if result.has_forms:
            print(f"è´­ä¹°è¡¨å•: {len(result.forms)} ä¸ª")


async def website_structure_analysis():
    """ç½‘ç«™ç»“æ„åˆ†æ"""
    print("\nğŸ—ï¸ ç½‘ç«™ç»“æ„åˆ†æ")
    print("-" * 40)

    config = ExtractionConfig(
        extract_text=True,
        extract_links=True,
        extract_structured_data=True,
        max_links=50,
    )

    async with Spider(extraction_config=config) as spider:
        result = await spider.crawl("https://httpbin.org/html")

        print(f"åˆ†æç½‘ç«™: {result.url}")
        print(f"é¡µé¢ç»“æ„:")
        print(f"  æ–‡æœ¬å†…å®¹: {result.content_length} å­—ç¬¦")
        print(f"  æ€»é“¾æ¥æ•°: {result.total_links}")

        # é“¾æ¥ç»“æ„åˆ†æ
        if result.has_links:
            nav_links = result.get_navigation_links()
            content_links = result.get_content_links()

            print(f"  å¯¼èˆªé“¾æ¥: {len(nav_links)}")
            print(f"  å†…å®¹é“¾æ¥: {len(content_links)}")

        # é¡µé¢å…ƒæ•°æ®
        print(f"é¡µé¢å…ƒæ•°æ®:")
        print(f"  æ ‡é¢˜: {result.metadata.title}")
        print(f"  æè¿°: {result.metadata.description}")
        print(f"  è¯­è¨€: {result.metadata.language}")


async def content_monitoring():
    """å†…å®¹ç›‘æ§ç¤ºä¾‹"""
    print("\nğŸ‘ï¸ å†…å®¹ç›‘æ§")
    print("-" * 40)

    # ç›‘æ§å¤šä¸ªé¡µé¢çš„å˜åŒ–
    urls = ["https://example.com", "https://httpbin.org/html"]

    config = ExtractionConfig(extract_text=True, extract_metadata=True)

    async with Spider(extraction_config=config) as spider:
        results = await spider.crawl_batch(urls, max_concurrent=3)

        print("ç›‘æ§ç»“æœ:")
        for result in results:
            if result.success:
                print(f"âœ… {result.url}")
                print(f"   å†…å®¹: {result.content_length} å­—ç¬¦")
                print(f"   æ›´æ–°æ—¶é—´: {result.timestamp}")
            else:
                print(f"âŒ {result.url} - è®¿é—®å¤±è´¥")


async def data_extraction_with_selectors():
    """ä½¿ç”¨é€‰æ‹©å™¨è¿›è¡Œç²¾ç¡®æ•°æ®æå–"""
    print("\nğŸ¯ ç²¾ç¡®æ•°æ®æå–")
    print("-" * 40)

    async with Spider() as spider:
        # æå–ç‰¹å®šçš„é¡µé¢å…ƒç´ 
        selectors = {
            "main_title": "h1, title",
            "navigation": "nav a, .nav a",
            "content_paragraphs": "p",
            "external_links": "a[href^='http']:not([href*='example.com'])",
        }

        result = await spider.extract_with_selectors("https://example.com", selectors)

        print(f"é€‰æ‹©å™¨æå–ç»“æœ:")
        for field, data in result.structured_data.items():
            if data:
                if isinstance(data, list):
                    print(f"  {field}: {len(data)} ä¸ªå…ƒç´ ")
                else:
                    preview = (
                        str(data)[:50] + "..." if len(str(data)) > 50 else str(data)
                    )
                    print(f"  {field}: {preview}")
            else:
                print(f"  {field}: æœªæ‰¾åˆ°")


async def performance_optimized_crawling():
    """æ€§èƒ½ä¼˜åŒ–çš„çˆ¬å–ç¤ºä¾‹"""
    print("\nâš¡ æ€§èƒ½ä¼˜åŒ–çˆ¬å–")
    print("-" * 40)

    # ä¼˜åŒ–é…ç½®ï¼šåªæå–å¿…è¦çš„æ•°æ®
    config = ExtractionConfig(
        extract_text=True,
        extract_links=True,
        max_links=10,  # é™åˆ¶é“¾æ¥æ•°é‡
        extract_metadata=True,
    )

    # ä¼˜åŒ–çˆ¬è™«é…ç½®
    spider_config = SpiderConfig(
        headless=True,
        stealth_mode=False,  # å…³é—­éšèº«æ¨¡å¼ä»¥æé«˜é€Ÿåº¦
        timeout=10000,  # è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
        random_delay=False,  # å…³é—­éšæœºå»¶è¿Ÿ
    )

    urls = ["https://example.com", "https://httpbin.org/html"]

    import time

    start_time = time.time()

    async with Spider(config, spider_config) as spider:
        results = await spider.crawl_batch(urls, max_concurrent=5)

    duration = time.time() - start_time

    success_count = sum(1 for r in results if r.success)
    total_content = sum(r.content_length for r in results if r.success)

    print(f"æ€§èƒ½ç»Ÿè®¡:")
    print(f"  å¤„ç†æ—¶é—´: {duration:.2f} ç§’")
    print(f"  æˆåŠŸç‡: {success_count}/{len(urls)}")
    print(f"  æ€»å†…å®¹: {total_content} å­—ç¬¦")
    print(f"  å¹³å‡é€Ÿåº¦: {total_content/duration:.0f} å­—ç¬¦/ç§’")


async def main():
    """è¿è¡Œæ‰€æœ‰å®ç”¨æ¡ˆä¾‹"""
    print("ğŸ”§ Xpidy å®ç”¨æ¡ˆä¾‹æ¼”ç¤º")
    print("=" * 50)

    try:
        await news_article_extraction()
        await e_commerce_product_analysis()
        await website_structure_analysis()
        await content_monitoring()
        await data_extraction_with_selectors()
        await performance_optimized_crawling()

        print("\nğŸ‰ æ‰€æœ‰å®ç”¨æ¡ˆä¾‹æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ è¿™äº›æ¡ˆä¾‹å±•ç¤ºäº† Xpidy åœ¨ä»¥ä¸‹åœºæ™¯çš„åº”ç”¨:")
        print("   - æ–°é—»æ–‡ç« å†…å®¹æå–")
        print("   - ç”µå•†äº§å“ä¿¡æ¯åˆ†æ")
        print("   - ç½‘ç«™ç»“æ„åˆ†æ")
        print("   - å†…å®¹å˜åŒ–ç›‘æ§")
        print("   - ç²¾ç¡®æ•°æ®æå–")
        print("   - æ€§èƒ½ä¼˜åŒ–ç­–ç•¥")

    except Exception as e:
        print(f"âŒ æ¡ˆä¾‹è¿è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
